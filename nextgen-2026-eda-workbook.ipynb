{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coderuntheworld/csiro-nextgen-2026-phd/blob/main/nextgen-2026-eda-workbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INTRO TEXT\n",
        "---\n",
        "**OVERVIEW**\n",
        "\n",
        "Exploratory Data Analysis (EDA) is the process of examining a dataset to:\n",
        "* develop an understanding of the data's structure and characteristics\n",
        "* identify patterns and anomalies\n",
        "* lay the groundwork for subsequent modelling or further analysis steps\n",
        "\n",
        "Steps (noting that this is often an iterative process):\n",
        "1. Clarify the problem and objectives\n",
        "2. Familiarise yourself with the dataset\n",
        "3. Assess the quality of the data\n",
        "4. Examine individual variables\n",
        "5. Investigate relationships between variables\n",
        "6. Interpret findings in the context of the problem\n",
        "7. Clean and transform the data as required\n",
        "---\n",
        "**QUESTIONNAIRE SUBSET^**\n",
        "\n",
        "Music Preferences\n",
        "\n",
        "* **I enjoy listening to music:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I prefer:** *Slow paced music 1-2-3-4-5 Fast paced music*\n",
        "* **Dance, Disco, Funk:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Folk music:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Country:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Classical:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Musicals:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Pop:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Rock:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Metal, Hard rock:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Punk:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Hip hop, Rap:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Reggae, Ska:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Swing, Jazz:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Rock n Roll:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Alternative music:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Latin:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Techno, Trance:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "* **Opera:** *Don't enjoy at all 1-2-3-4-5 Enjoy very much*\n",
        "\n",
        "Personality Traits, Views On Life & Opinions\n",
        "* **I take notice of what goes on around me:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I try to do tasks as soon as possible and not leave them until last minute:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I always make a list so I don't forget anything:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I often study or work even in my spare time:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I look at things from all different angles before I go ahead:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I believe that bad people will suffer one day and good people will be rewarded:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I am reliable at work and always complete all tasks given to me:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I always keep my promises:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I can fall for someone very quickly and then completely lose interest:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I would rather have lots of friends than lots of money:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I always try to be the funniest one:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I can be two faced sometimes:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I damaged things in the past when angry:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I take my time to make decisions:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I always try to vote in elections:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I often think about and regret the decisions I make:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I can tell if people listen to me or not when I talk to them:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I am a hypochondriac:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I am emphathetic person:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I eat because I have to. I don't enjoy food and eat as fast as I can:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I try to give as much as I can to other people at Christmas:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I don't like seeing animals suffering:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I look after things I have borrowed from others:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I feel lonely in life:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I used to cheat at school:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I worry about my health:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I wish I could change the past because of the things I have done:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I believe in God:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I always have good dreams:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I always give to charity:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I have lots of friends:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **Timekeeping:** *I am often early. - I am always on time. - I am often running late.*\n",
        "* **Do you lie to others?:** *Never. - Only to avoid hurting someone. - Sometimes. - Everytime it suits me.*\n",
        "* **I am very patient:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I can quickly adapt to a new environment:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **My moods change quickly:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I am well mannered and I look after my appearance:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I enjoy meeting new people:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I always let other people know about my achievements:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I think carefully before answering any important letters:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I enjoy childrens' company:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I am not afraid to give my opinion if I feel strongly about something:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I can get angry very easily:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I always make sure I connect with the right people:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I have to be well prepared before public speaking:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I will find a fault in myself if people don't like me:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I cry when I feel down or things don't go the right way:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I am 100% happy with my life:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I am always full of life and energy:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I prefer big dangerous dogs to smaller, calmer dogs:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I believe all my personality traits are positive:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **If I find something the doesn't belong to me I will hand it in:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I find it very difficult to get up in the morning:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I have many different hobbies and interests:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I always listen to my parents' advice:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **I enjoy taking part in surveys:** *Strongly disagree 1-2-3-4-5 Strongly agree*\n",
        "* **How much time do you spend online?:** *No time at all - Less than an hour a day - Few hours a day - Most of the day*\n",
        "\n",
        "*^This is a subset of a full questionnaire that was asked to statistics students in 2013. For the purpose of this workshop, we will be using a subset of the questionnaire and dataset and applying it to a hypthetical scenario. For more information on the original dataset and purpose, see https://www.kaggle.com/datasets/miroslavsabo/young-people-survey*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "r1aKuSkuPE0z"
      },
      "id": "r1aKuSkuPE0z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SET UP\n",
        "The following code sets up the workbook. It imports the required libraries, loads the dataset and defines the functions that will be used in the workshop."
      ],
      "metadata": {
        "id": "HivnATOatRkC"
      },
      "id": "HivnATOatRkC"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title INITIALISE\n",
        "\n",
        "# This code block imports libraries, loads the dataset and sets up functions specific for this workshop.\n",
        "\n",
        "# import libraries\n",
        "!pip install factor_analyzer\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from math import ceil, floor\n",
        "from scipy.stats import kruskal\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from factor_analyzer.factor_analyzer import calculate_kmo\n",
        "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#set print settings\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "try:\n",
        "  # load data source\n",
        "  responses_url = 'https://raw.githubusercontent.com/nextgenerationgraduatesprogram/nextgen2026-eda-workshop/refs/heads/summary/responses.csv'\n",
        "  df_raw = pd.read_csv(responses_url)\n",
        "  columns_url = 'https://raw.githubusercontent.com/nextgenerationgraduatesprogram/nextgen2026-eda-workshop/refs/heads/summary/columns.csv'\n",
        "  df_cols = pd.read_csv(columns_url)\n",
        "\n",
        "  # define variable groups\n",
        "  music_cols = list(df_raw.columns[0:19])\n",
        "  movies_cols = list(df_raw.columns[19:31])\n",
        "  hobbies_cols = list(df_raw.columns[31:63])\n",
        "  phobias_cols = list(df_raw.columns[63:73])\n",
        "  health_cols = list(df_raw.columns[73:76])\n",
        "  personality_cols = list(df_raw.columns[76:133])\n",
        "  spending_cols = list(df_raw.columns[133:140])\n",
        "  demographics_cols = list(df_raw.columns[140:150])\n",
        "\n",
        "  # make a copy of the raw data, subsetted to the scenario data\n",
        "  df = df_raw[music_cols + personality_cols].copy()\n",
        "  print(\"Dataset loaded successfully.\")\n",
        "except:\n",
        "  print(\"ERROR: Dataset load failed.\")\n",
        "\n",
        "# define dataset summary\n",
        "def dataset_summary(df):\n",
        "  print('\\nSHAPE:')\n",
        "  print(f'Number of observations: {df.shape[0]}')\n",
        "  print(f'Number of variables: {df.shape[1]}')\n",
        "\n",
        "  print('\\nCOLUMN HEADERS:')\n",
        "  print('(displayed in order of the dataset and corresponds with the order of the questions)')\n",
        "  print(df.columns)\n",
        "\n",
        "  print('\\nDATA TYPE & MISSING VALUES:')\n",
        "  print('(displayed in descending order of missing percentage)')\n",
        "  quality_summary = pd.DataFrame({\n",
        "      'dtype': df.dtypes,\n",
        "      'missing': df.isnull().sum(),\n",
        "      'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)\n",
        "      }).sort_values('missing_pct', ascending=False)\n",
        "  print(quality_summary)\n",
        "\n",
        "  print('\\nDISTRIBUTIONS:')\n",
        "  print('(each graph displays the possible values in ascending order along the x axis)')\n",
        "  num_cols = 5\n",
        "  num_rows = ceil(len(df.columns)/num_cols)\n",
        "  fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 15))\n",
        "  plt.tight_layout()\n",
        "  axs_flat = axs.flatten() if isinstance(axs, np.ndarray) else [axs]\n",
        "  for idx, k in enumerate(df.columns):\n",
        "    ax = axs_flat[idx]\n",
        "    ax.set_title(k)\n",
        "    x = list(df[k].value_counts().index)\n",
        "    y = df[k].value_counts().values\n",
        "    ax.bar(sorted(x),[int(n) for _,n in sorted(zip(x,y))])\n",
        "    ax.set_xticks(sorted(x), sorted(x), ha='center')\n",
        "  for i in range(len(df.columns), len(axs_flat)):\n",
        "      fig.delaxes(axs_flat[i])\n",
        "  plt.show()\n",
        "\n",
        "  print('\\nUNIQUE VALUES & FREQ (excl. missing values):')\n",
        "  for col in df.columns:\n",
        "    print(f'{df[col].value_counts()}\\n')\n",
        "\n",
        "  print('\\nSAMPLE FIRST 10 ROWS:')\n",
        "  print(df.head(10))\n",
        "\n",
        "# produce correlation matrix\n",
        "def generate_correlation_matrix(df, corr_setting):\n",
        "  print(f'\\nCORRELATION MATRIX HEATMAP ({corr_setting}, method: Spearman Rank):')\n",
        "  df_for_corr = df.select_dtypes(include='number')\n",
        "  correlation_matrix = df_for_corr.corr(method='spearman')\n",
        "  figure_name = 'Correlation matrix'\n",
        "  fig, ax = plt.subplots(figsize=(40,40))\n",
        "  sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm_r', ax=ax)\n",
        "  plt.title(figure_name, wrap=True, horizontalalignment='center', fontsize=8)\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "  corr_unstack = correlation_matrix.unstack().abs()\n",
        "  corr_unstack = corr_unstack.sort_values(kind=\"quicksort\", ascending=True)\n",
        "  top_50_pairs = pd.DataFrame(corr_unstack[-(df.shape[1]+101):-(df.shape[1]+1)]).sort_values(by=0, ascending=False).iloc[::2,:].reset_index()\n",
        "  top_50_pairs.columns = ['var1', 'var2', 'corr_abs']\n",
        "  print('\\nCorrelation pairs (top 50):')\n",
        "  print(top_50_pairs)\n",
        "  return correlation_matrix\n",
        "\n",
        "# produce contingency tables\n",
        "def generate_contigency_tables_all(df, cont_setting):\n",
        "  print(f'\\n\\nCONTINGENCY TABLES ({cont_setting}):')\n",
        "  for i, col in enumerate(df.columns[:-1]):\n",
        "    j = i + 1\n",
        "    while j < len(df.columns):\n",
        "      contingency1 = pd.crosstab(df.iloc[:,i], df.iloc[:,j], normalize=\"index\") * 100\n",
        "      print(f'\\n{df.columns[i]} v {df.columns[j]} 1. Sum of row = 100%')\n",
        "      print(contingency1)\n",
        "      contingency2 = pd.crosstab(df.iloc[:,i], df.iloc[:,j], normalize=\"columns\") * 100\n",
        "      print(f'\\n{df.columns[i]} v {df.columns[j]} 2. Sum of column = 100%')\n",
        "      print(contingency2)\n",
        "      contingency3 = pd.crosstab(df.iloc[:,i], df.iloc[:,j], normalize=\"all\") * 100\n",
        "      print(f'\\n{df.columns[i]} v {df.columns[j]} 3. Sum of all cells = 100%')\n",
        "      print(contingency3)\n",
        "      j = j + 1\n",
        "\n",
        "# produce bubble plot\n",
        "def generate_bubble_plot_kw(df_t):\n",
        "  print('\\nBUBBLE PLOT Kruskal-Wallis H-test  (one v rest):')\n",
        "  kw_results = []\n",
        "  categories = df_t[target_variable_name].unique()\n",
        "  for feature in x:\n",
        "      for cat in categories:\n",
        "          group1 = df_t[df_t[target_variable_name] == cat][feature].dropna()\n",
        "          group2 = df_t[df_t[target_variable_name] != cat][feature].dropna()\n",
        "          if len(group1) > 1 and len(group2) > 1:\n",
        "              stat, p = kruskal(group1, group2)\n",
        "          else:\n",
        "              p = np.nan\n",
        "          kw_results.append([feature, cat, p])\n",
        "  kw_df = pd.DataFrame(kw_results, columns=[\"Feature\", \"Category\", \"p_value\"])\n",
        "  kw_df = kw_df.dropna(subset=[\"p_value\"])\n",
        "  min_size, max_size = 50, 800\n",
        "  kw_df[\"neg_log_p\"] = -np.log10(kw_df[\"p_value\"].clip(lower=1e-10))\n",
        "  log_min, log_max = kw_df[\"neg_log_p\"].min(), kw_df[\"neg_log_p\"].max()\n",
        "  kw_df[\"size_norm\"] = (kw_df[\"neg_log_p\"] - log_min) / (log_max - log_min + 1e-9)\n",
        "  kw_df[\"bubble_size\"] = -np.log10(kw_df[\"p_value\"].replace(0, 1e-10))\n",
        "  bright_blue      = (0.0, 0.0, 1.0, 1.0)\n",
        "  transparent_blue = (0.2, 0.4, 1.0, 0.1)\n",
        "  blue_cmap = LinearSegmentedColormap.from_list(\n",
        "      \"blue_cmap\",\n",
        "      [bright_blue, transparent_blue]\n",
        "  )\n",
        "  kw_df[\"Category\"] = pd.Categorical(kw_df[\"Category\"], categories=categories, ordered=True)\n",
        "  cat_counts = df_t[target_variable_name].value_counts().to_dict()\n",
        "  x_labels = [f\"{cat} ({cat_counts.get(cat, 0)})\" for cat in categories]\n",
        "  plt.figure(figsize=(12, max(6, len(x) * 0.35)))\n",
        "  ax = sns.scatterplot(\n",
        "      data=kw_df,\n",
        "      x=\"Category\",\n",
        "      y=\"Feature\",\n",
        "      size=\"bubble_size\",\n",
        "      hue=\"p_value\",\n",
        "      palette=blue_cmap,\n",
        "      hue_norm=(kw_df[\"p_value\"].min(), kw_df[\"p_value\"].max()),\n",
        "      sizes=(min_size, max_size),\n",
        "      legend=\"brief\"\n",
        "  )\n",
        "  for i, row in kw_df.iterrows():\n",
        "    if row[\"p_value\"] < 0.05:\n",
        "        ax.text(\n",
        "            x=row[\"Category\"],\n",
        "            y=row[\"Feature\"],\n",
        "            s=f\"{row['p_value']:.3f}\",\n",
        "            color=\"white\",\n",
        "            fontsize=6,\n",
        "            ha=\"center\",\n",
        "            va=\"center\"\n",
        "        )\n",
        "  ax.set_xticks(range(len(categories)))\n",
        "  ax.set_xticklabels(x_labels, rotation=45, ha='right', fontsize=7)\n",
        "  ax.set_yticks(range(len(x)))\n",
        "  ax.set_yticklabels(x, fontsize=7)\n",
        "  plt.title(\"\")\n",
        "  plt.xlabel(\"Target Categories (n)\")\n",
        "  plt.ylabel(\"Features\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "# map feature col to value in target\n",
        "def generate_contigency_tables_xy(df, cont_setting):\n",
        "  music_value_dict = {}\n",
        "  for key in music_cols[2:19]:\n",
        "    music_value_dict[key] = str(key).lower()\n",
        "  print(f'\\n\\nCONTINGENCY TABLES ({cont_setting}):')\n",
        "  cat_counts = df[target_variable_name].value_counts().to_dict()\n",
        "  cat_counts_small = [key for key, value in cat_counts.items() if value < 10]\n",
        "  cat_counts_small_mapped = [cat for cat, smallstyle in music_value_dict.items() if smallstyle in cat_counts_small]\n",
        "  if len(cat_counts_small_mapped) > 0:\n",
        "    for maincol in cat_counts_small_mapped:\n",
        "      for j, compcol in enumerate(df.columns[:-1]):\n",
        "        if maincol != compcol:\n",
        "          n1 = f' ({cat_counts.get(str(maincol).lower(), 0)})'\n",
        "          n2 = f' ({cat_counts.get(str(compcol).lower(), 0)})'\n",
        "          contingency1 = pd.crosstab(df[maincol], df[compcol], normalize=\"index\") * 100\n",
        "          print(f'\\n{maincol}{n1} v {compcol}{n2} 1. Sum of row = 100%')\n",
        "          print(contingency1)\n",
        "        j = j + 1\n",
        "  else:\n",
        "    print('None.')\n",
        "\n",
        "# define relationships summary\n",
        "def explore_relationships(df, subset):\n",
        "  if subset == \"all\":\n",
        "    df_for_corr = df.select_dtypes(include='number')\n",
        "    corr_setting = 'all numeric variables'\n",
        "    cr = generate_correlation_matrix(df_for_corr, corr_setting)\n",
        "    df_for_cont = df.select_dtypes(exclude='number')\n",
        "    cont_setting = 'all categorical variables'\n",
        "    generate_contigency_tables_all(df_for_cont, cont_setting)\n",
        "  elif subset == \"x,y\":\n",
        "    df_for_corr = df[x].select_dtypes(include='number')\n",
        "    corr_setting = 'features only'\n",
        "    cr = generate_correlation_matrix(df_for_corr, corr_setting)\n",
        "    df_for_cont = df[(music_cols[2:19] + [target_variable_name])]\n",
        "    cont_setting = 'selected variables where target is less than 10'\n",
        "    generate_contigency_tables_xy(df_for_cont, cont_setting)\n",
        "    generate_bubble_plot_kw(df)\n",
        "  else:\n",
        "    print('Please input \"all\" or \"x,y\" into the second argument of the function to produce a summary.')\n",
        "\n",
        "# determine the mapping of the non-numeric feature values to numeric\n",
        "Punctuality_dict = {\n",
        "    \"i am often running late\": \"1\",\n",
        "    \"i am always on time\": \"2\",\n",
        "    \"i am often early\": \"3\"\n",
        "    }\n",
        "Lying_dict = {\n",
        "    \"never\": \"1\",\n",
        "    \"only to avoid hurting someone\": \"2\",\n",
        "    \"sometimes\": \"3\",\n",
        "    \"everytime it suits me\": \"4\"\n",
        "    }\n",
        "Internet_usage_dict = {\n",
        "    \"no time at all\": \"1\",\n",
        "    \"less than an hour a day\": \"2\",\n",
        "    \"few hours a day\": \"3\",\n",
        "    \"most of the day\": \"4\"\n",
        "    }\n",
        "\n",
        "# determine the mapping of target values with less than 10\n",
        "music_pref_dict = {\n",
        "    'rock n roll': 'rock',\n",
        "    'reggae, ska': 'rock'\n",
        "    }\n",
        "\n",
        "def map_cat_cols_to_num(df_t):\n",
        "  print('\\nFeatures remapped:')\n",
        "  df_t['Lying'] = df_t['Lying'].map(Lying_dict).astype(float)\n",
        "  df_t['Punctuality'] = df_t['Punctuality'].map(Punctuality_dict).astype(float)\n",
        "  df_t['Internet usage'] = df_t['Internet usage'].map(Internet_usage_dict).astype(float)\n",
        "  print('Lying: \\n   ' + str(Lying_dict))\n",
        "  print('Punctuality: \\n   ' + str(Punctuality_dict))\n",
        "  print('Internet usage: \\n   ' + str(Internet_usage_dict))\n",
        "  return df_t\n",
        "\n",
        "# define how the target variable is produced\n",
        "# NB: 'Music' -> I enjoy listening to music: Strongly disagree 1-2-3-4-5 Strongly agree\n",
        "def create_target_variable(df_t):\n",
        "  df_t[target_variable_name] = np.where(df_t['Music']>3.0,df_t[music_cols[2:]].idxmax(axis=1).str.lower(),'')\n",
        "  df_t[target_variable_name] = np.where(df_t['Music']<=3.0,'does not enjoy music',df_t[target_variable_name])\n",
        "  y = [target_variable_name]\n",
        "  print('\\nTarget variable (y): ' + str(y))\n",
        "  print('First 10 rows:')\n",
        "  print(df_t[y].head(10))\n",
        "  return y, df_t\n",
        "\n",
        "# set which variables will be used for as the features for personality traits\n",
        "def determine_feature_cols(df_t):\n",
        "  x = personality_cols\n",
        "  print('\\nFeature variable/s (x): \\n' + str(x))\n",
        "  return x\n",
        "\n",
        "# determine what to do with missing values in the target variable\n",
        "def treat_y_missing(df_t):\n",
        "  df_t = df_t.loc[df_t[target_variable_name]!='',:]\n",
        "  print('\\nMissing values in target have been dropped.')\n",
        "  return df_t\n",
        "\n",
        "# determine what to do with missing values in the features\n",
        "def treat_x_missing(df_t, type_str):\n",
        "  if type_str == \"mode\":\n",
        "    df_t = df_t.fillna(df_t.mode().iloc[0])\n",
        "    print('\\nMissing values in features have been imputed with the mode.')\n",
        "  else:\n",
        "    df_t = df_t.dropna()\n",
        "    print('\\nMissing values in features have been dropped.')\n",
        "  return df_t\n",
        "\n",
        "# determine what to do with missing values in the features\n",
        "def treat_y_smallsize(df_t):\n",
        "  df_t[target_variable_name] = df_t[target_variable_name].map(music_pref_dict).fillna(df_t[target_variable_name])\n",
        "  print('\\nTarget variable remapped: \\n   ' + str(music_pref_dict))\n",
        "  return df_t\n",
        "\n",
        "# define KMO test\n",
        "def run_kmo(df):\n",
        "  kmo_all, kmo_model = calculate_kmo(df)\n",
        "  return kmo_all, kmo_model\n",
        "\n",
        "# define bartletts test\n",
        "def run_bartletts(df):\n",
        "  cr = df.corr(method='spearman')\n",
        "  chi2, p_value = calculate_bartlett_sphericity(cr)\n",
        "  p = cr.shape[1]\n",
        "  dgf = int(p * (p - 1) / 2)\n",
        "  return chi2, dgf, p_value, cr\n",
        "\n",
        "# define pca1\n",
        "def run_pca(cr):\n",
        "  eigenvalues, eigenvectors = np.linalg.eig(cr)\n",
        "  eig_pairs = [(np.abs(eigenvalues[i]), eigenvectors[:, i]) for i in range(len(eigenvalues))]\n",
        "  eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "  total_variance = sum(eigenvalues)\n",
        "  explained_variance_ratio = [(i / total_variance) for i in sorted(eigenvalues, reverse=True)]\n",
        "  cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o', linestyle='--', color='blue')\n",
        "  plt.title('Scree Plot (Eigenvalues vs. Principal Component)')\n",
        "  plt.xlabel('Principal Component Number')\n",
        "  plt.ylabel('Eigenvalue (Explained Variance)')\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "  plt.title('Cumulative explained variance by number of components')\n",
        "  xint = range(1, len(cumulative_explained_variance) + 1)\n",
        "  plt.plot(xint, cumulative_explained_variance)\n",
        "  plt.xlabel(\"Number of components\")\n",
        "  plt.ylabel(\"Cumulative explained variance\")\n",
        "  plt.show()\n",
        "  print(f'\\nTotal number of eigenvalues: {str(total_variance)}\\n')\n",
        "  for i in range(len(eigenvalues)):\n",
        "    print(f'First {str(i+1)} eigenvalues explain {str(round(sum(explained_variance_ratio[0:(1+i)]),5))}%')\n",
        "  print(\"\\nEigenvalues:\\n\", eigenvalues)\n",
        "  print(\"\\nEigenvectors:\\n\", eigenvectors)\n",
        "  print(\"\\nExplained Variance Ratio:\\n\", explained_variance_ratio)\n",
        "  print(\"\\nCumulative Explained Variance:\\n\", cumulative_explained_variance)\n",
        "  return eigenvalues, eigenvectors\n",
        "\n",
        "# define pca2\n",
        "def create_pca_scatterplots(df_features, square):\n",
        "  print(f'\\nPrincipal Component Comparison ({str(square)}):\\n')\n",
        "  df_features_scaled = pd.DataFrame(StandardScaler().fit_transform(df_features),columns=df_features.columns)\n",
        "  for i in range(len(eigenvectors)):\n",
        "    df_features_scaled['PC' + str(i+1)] = df_features_scaled[x].mul(np.array(eigenvectors[i]), axis=1)[x].sum(axis=1)\n",
        "  colour_labels = df_t[target_variable_name].unique()\n",
        "  colour_dict = {colour_labels[i]: i for i in range(len(colour_labels))}\n",
        "  fig, axes = plt.subplots(nrows=square, ncols=square, figsize=(22, 22))\n",
        "  axes = axes.flatten()\n",
        "  n = -1\n",
        "  for i in range(square):\n",
        "    for j in range(square):\n",
        "      n = n + 1\n",
        "      axes[n].scatter(df_features_scaled[['PC' + str(i+1)]], df_features_scaled[['PC' + str(j+1)]], c=df_t[target_variable_name].map(colour_dict), cmap='plasma', alpha=0.5, edgecolors='none')\n",
        "      axes[n].set_xlabel('PC' + str(i+1))\n",
        "      axes[n].set_ylabel('PC' + str(j+1))\n",
        "      axes[n].grid(True)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  print(f'\\nPrincipal Component Top 10 Weights ({str(square)}):\\n')\n",
        "  cols = df_features.columns\n",
        "  for idx, (a, b) in enumerate(zip(eigenvalues, eigenvectors)):\n",
        "    if idx < square:\n",
        "      print(f'\\n{str(idx+1)} [{str(a)}]:\\n')\n",
        "      featlist = pd.DataFrame(zip(cols, b), columns=['feature','weight'])\n",
        "      featlist['weight_abs'] = abs(featlist['weight'])\n",
        "      featlist = featlist.sort_values(by='weight')\n",
        "      print(featlist[:10])\n",
        "\n",
        "# try random forest model\n",
        "def run_random_forest(df_t):\n",
        "  print('Random Forest Classification model (n_estimators = 50):')\n",
        "  X = df_t[x]\n",
        "  Y = df_t[target_variable_name]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.6, test_size=0.4, stratify=Y, random_state=12345)\n",
        "  rf_model = RandomForestClassifier(n_estimators=50, random_state=12345)\n",
        "  rf_model.fit(X_train, y_train)\n",
        "  predictions = rf_model.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, predictions)\n",
        "  print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "9RjF-x_GLoO_",
        "cellView": "form",
        "outputId": "d118cda4-3a6c-4f53-c976-7c868d8ee345",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9RjF-x_GLoO_",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: factor_analyzer in /usr/local/lib/python3.12/dist-packages (0.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from factor_analyzer) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from factor_analyzer) (1.16.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from factor_analyzer) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from factor_analyzer) (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->factor_analyzer) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->factor_analyzer) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->factor_analyzer) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->factor_analyzer) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->factor_analyzer) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->factor_analyzer) (1.17.0)\n",
            "Dataset loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# START OF WORKBOOK\n",
        "The subsequent code cells are part of the workbook that we will work through for this workshop."
      ],
      "metadata": {
        "id": "hKntZnE1zhAe"
      },
      "id": "hKntZnE1zhAe"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RESEARCH QUESTION\n",
        "\n",
        "research_questions = {\n",
        "    'primary': 'Can we create an accurate model that predicts a user\\'s favourite music genre based on personality traits?',\n",
        "    'sub_questions': [\n",
        "        'How do we define \\'favourite music genre\\'?',\n",
        "        'How do we define \\'personality traits\\'?',\n",
        "        'Is the quality of the data suitable for further analysis and modelling?',\n",
        "        'Which personality traits are most/least related to favourite music genre?',\n",
        "        'Are there early indicators that a predictive model based on this dataset would perform well?'\n",
        "    ]\n",
        "}\n",
        "print(f'Primary question:\\n  {research_questions['primary']}\\n')\n",
        "print('Guide questions:')\n",
        "for idx, i  in enumerate(research_questions['sub_questions']):\n",
        "  print(f'  {str(idx+1)}. {i}')"
      ],
      "metadata": {
        "id": "oerweho6vuFy",
        "cellView": "form",
        "outputId": "8a71561a-025a-45d0-df48-f9d2652d298f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oerweho6vuFy",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primary question:\n",
            "  Can we create an accurate model that predicts a user's favourite music genre based on personality traits?\n",
            "\n",
            "Guide questions:\n",
            "  1. How do we define 'favourite music genre'?\n",
            "  2. How do we define 'personality traits'?\n",
            "  3. Is the quality of the data suitable for further analysis and modelling?\n",
            "  4. Which personality traits are most/least related to favourite music genre?\n",
            "  5. Are there early indicators that a predictive model based on this dataset would perform well?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RUN INITIAL DATASET SUMMARY\n",
        "\n",
        "# The 'dataset_summary' function produces a high level output of key characteristics of the dataset.\n",
        "# The function requires the dataframe as the input.\n",
        "\n",
        "dataset_summary(df)"
      ],
      "metadata": {
        "id": "XDhRtXvhwoOm",
        "collapsed": true,
        "outputId": "48715da0-a433-48bd-b95d-f931933c4bc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XDhRtXvhwoOm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SHAPE:\n",
            "Number of observations: 1010\n",
            "Number of variables: 76\n",
            "\n",
            "COLUMN HEADERS:\n",
            "(displayed in order of the dataset and corresponds with the order of the questions)\n",
            "Index(['Music', 'Slow songs or fast songs', 'Dance', 'Folk', 'Country', 'Classical music', 'Musical', 'Pop', 'Rock', 'Metal or Hardrock', 'Punk', 'Hiphop, Rap', 'Reggae, Ska', 'Swing, Jazz', 'Rock n roll', 'Alternative', 'Latino', 'Techno, Trance', 'Opera', 'Daily events', 'Prioritising workload', 'Writing notes', 'Workaholism', 'Thinking ahead', 'Final judgement', 'Reliability', 'Keeping promises', 'Loss of interest', 'Friends versus money', 'Funniness', 'Fake', 'Criminal damage', 'Decision making', 'Elections', 'Self-criticism', 'Judgment calls', 'Hypochondria', 'Empathy', 'Eating to survive', 'Giving', 'Compassion to animals', 'Borrowed stuff', 'Loneliness', 'Cheating in school', 'Health', 'Changing the past', 'God', 'Dreams', 'Charity', 'Number of friends', 'Punctuality', 'Lying', 'Waiting', 'New environment', 'Mood swings', 'Appearence and gestures', 'Socializing', 'Achievements', 'Responding to a serious letter', 'Children', 'Assertiveness', 'Getting angry',\n",
            "       'Knowing the right people', 'Public speaking', 'Unpopularity', 'Life struggles', 'Happiness in life', 'Energy levels', 'Small - big dogs', 'Personality', 'Finding lost valuables', 'Getting up', 'Interests or hobbies', 'Parents' advice', 'Questionnaires or polls', 'Internet usage'],\n",
            "      dtype='object')\n",
            "\n",
            "DATA TYPE & MISSING VALUES:\n",
            "(displayed in descending order of missing percentage)\n",
            "                                  dtype  missing  missing_pct\n",
            "Punk                            float64        8         0.79\n",
            "Latino                          float64        8         0.79\n",
            "Reggae, Ska                     float64        7         0.69\n",
            "Classical music                 float64        7         0.69\n",
            "Criminal damage                 float64        7         0.69\n",
            "Final judgement                 float64        7         0.69\n",
            "Techno, Trance                  float64        7         0.69\n",
            "Daily events                    float64        7         0.69\n",
            "Rock n roll                     float64        7         0.69\n",
            "Alternative                     float64        7         0.69\n",
            "Compassion to animals           float64        7         0.69\n",
            "Rock                            float64        6         0.59\n",
            "Responding to a serious letter  float64        6         0.59\n",
            "Giving                          float64        6         0.59\n",
            "Friends versus money            float64        6         0.59\n",
            "Swing, Jazz                     float64        6         0.59\n",
            "Energy levels                   float64        5         0.50\n",
            "Workaholism                     float64        5         0.50\n",
            "Empathy                         float64        5         0.50\n",
            "Country                         float64        5         0.50\n",
            "Folk                            float64        5         0.50\n",
            "Getting up                      float64        5         0.50\n",
            "Socializing                     float64        5         0.50\n",
            "Self-criticism                  float64        5         0.50\n",
            "Prioritising workload           float64        5         0.50\n",
            "Cheating in school              float64        4         0.40\n",
            "Hiphop, Rap                     float64        4         0.40\n",
            "Dance                           float64        4         0.40\n",
            "Personality                     float64        4         0.40\n",
            "Small - big dogs                float64        4         0.40\n",
            "Finding lost valuables          float64        4         0.40\n",
            "Getting angry                   float64        4         0.40\n",
            "Reliability                     float64        4         0.40\n",
            "Hypochondria                    float64        4         0.40\n",
            "Decision making                 float64        4         0.40\n",
            "Funniness                       float64        4         0.40\n",
            "Mood swings                     float64        4         0.40\n",
            "Children                        float64        4         0.40\n",
            "Happiness in life               float64        4         0.40\n",
            "Questionnaires or polls         float64        4         0.40\n",
            "Judgment calls                  float64        4         0.40\n",
            "Loss of interest                float64        4         0.40\n",
            "Music                           float64        3         0.30\n",
            "Metal or Hardrock               float64        3         0.30\n",
            "Thinking ahead                  float64        3         0.30\n",
            "Interests or hobbies            float64        3         0.30\n",
            "Pop                             float64        3         0.30\n",
            "Unpopularity                    float64        3         0.30\n",
            "Charity                         float64        3         0.30\n",
            "Waiting                         float64        3         0.30\n",
            "Elections                       float64        3         0.30\n",
            "Writing notes                   float64        3         0.30\n",
            "Appearence and gestures         float64        3         0.30\n",
            "Life struggles                  float64        3         0.30\n",
            "Musical                         float64        2         0.20\n",
            "Slow songs or fast songs        float64        2         0.20\n",
            "Lying                            object        2         0.20\n",
            "Borrowed stuff                  float64        2         0.20\n",
            "Changing the past               float64        2         0.20\n",
            "Achievements                    float64        2         0.20\n",
            "Knowing the right people        float64        2         0.20\n",
            "Assertiveness                   float64        2         0.20\n",
            "Parents' advice                 float64        2         0.20\n",
            "Public speaking                 float64        2         0.20\n",
            "Punctuality                      object        2         0.20\n",
            "New environment                 float64        2         0.20\n",
            "God                             float64        2         0.20\n",
            "Opera                           float64        1         0.10\n",
            "Health                          float64        1         0.10\n",
            "Loneliness                      float64        1         0.10\n",
            "Fake                            float64        1         0.10\n",
            "Keeping promises                float64        1         0.10\n",
            "Eating to survive                 int64        0         0.00\n",
            "Number of friends                 int64        0         0.00\n",
            "Dreams                            int64        0         0.00\n",
            "Internet usage                   object        0         0.00\n",
            "\n",
            "DISTRIBUTIONS:\n",
            "(each graph displays the possible values in ascending order along the x axis)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RUN SUMMARY OF RELATIONSHIPS\n",
        "\n",
        "# The 'explore_relationships' function provides a high-level summary of the relationships between the variables in the dataset.\n",
        "# The first argument of the function is the dataframe.\n",
        "# The second argument of the function is a subsetting setting. Options are \"all\" for all applicable variables, or \"x,y\" for when you have defined your features (x) and target variable (y).\n",
        "\n",
        "explore_relationships(df, \"all\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-Njmzu8mCeDd"
      },
      "id": "-Njmzu8mCeDd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RUN TRANSFORMS ON THE DATASET\n",
        "\n",
        "# This code blocks creates a new dataframe called 'df_t' and applies transforms to this new dataset.\n",
        "\n",
        "# make a copy of the dataset\n",
        "df_t = df.copy()\n",
        "\n",
        "# define the name of your target variable column\n",
        "target_variable_name = 'target_variable_name'\n",
        "\n",
        "# transform dataset\n",
        "y, df_t = create_target_variable(df_t)\n",
        "df_t = treat_y_missing(df_t)\n",
        "\n",
        "x = determine_feature_cols(df_t)\n",
        "df_t = map_cat_cols_to_num(df_t)\n",
        "df_t = treat_x_missing(df_t, \"drop\")\n",
        "\n",
        "#df_t = treat_y_smallsize(df_t)"
      ],
      "metadata": {
        "id": "o7_Fmav-j6Z1",
        "collapsed": true
      },
      "id": "o7_Fmav-j6Z1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RUN TESTS FOR PCA SUITABILITY\n",
        "\n",
        "# This code blocks runs KMO test and Bartlett's test for sphericity on the transformed dataset.\n",
        "# The Bartlett's test for sphericity function will also return the correlation matrix, cr, used in the test.\n",
        "\n",
        "# subset dataset to features only\n",
        "df_features = df_t.loc[:,x]\n",
        "\n",
        "# run kmo test\n",
        "kmo_all, kmo_model = run_kmo(df_features)\n",
        "print('Kaiser-Meyer-Olkin (KMO) test:')\n",
        "print(f'Individual KMO: {kmo_all}')\n",
        "print(f'Overall KMO: {kmo_model}')\n",
        "\n",
        "# run bartletts test for sphericity\n",
        "chi2, dgf, p_value, cr = run_bartletts(df_features)\n",
        "print('\\nBartlett test for sphericity:')\n",
        "print(f'Chi-square: {chi2}, Degrees of Freedom: {dgf}, P-value: {p_value}')"
      ],
      "metadata": {
        "id": "x1YVl8OT5reX",
        "collapsed": true
      },
      "id": "x1YVl8OT5reX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CONDUCT PCA\n",
        "\n",
        "# This code produces the eigenvalues and eigenvectors based on the inputted correlation matrix and produces scatterplots for n number of principal components,\n",
        "\n",
        "eigenvalues, eigenvectors = run_pca(cr)\n",
        "create_pca_scatterplots(df_features, 15)"
      ],
      "metadata": {
        "id": "gXoDOt195RVi",
        "collapsed": true
      },
      "id": "gXoDOt195RVi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END OF WORKBOOK"
      ],
      "metadata": {
        "id": "18-PCzmnzwGL"
      },
      "id": "18-PCzmnzwGL"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}